#!/usr/bin/env python3
"""
Prompt inspection utility for SQL RAG.

This script captures the exact prompts that ``answer_question_simple_gemini``
builds for different agents (default chat vs. ``@create``). It avoids any real
Gemini API calls by patching the Gemini client with a lightweight stub.

Usage examples:

    # Inspect both the default prompt and the @create (SQL generation) prompt
    python scripts/inspect_prompt_templates.py --question "Top products by revenue"

    # Inspect only the @create prompt with a specific FAISS index
    python scripts/inspect_prompt_templates.py \\
        --question "Top customers by order count" \\
        --agents create \\
        --index-name index_sample_queries_with_metadata_recovered

The script prints each captured prompt to stdout and can optionally write them
to files under the provided output directory.
"""

from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional
from unittest.mock import patch

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from data.app_data_loader import (  # noqa: E402
    DEFAULT_VECTOR_STORE,
    load_lookml_safe_join_map,
    load_schema_manager,
    load_vector_store,
)
from simple_rag_simple_gemini import answer_question_simple_gemini  # noqa: E402


class _PromptCaptureClient:
    """Lightweight stub that captures prompts instead of calling Gemini."""

    prompt_history: List[str] = []

    def __init__(self, model: Optional[str] = None, **_: Dict):  # pragma: no cover - simple stub
        self.model = model

    def invoke(self, prompt: str, *args, **kwargs) -> str:  # pragma: no cover - simple stub
        type(self).prompt_history = list(type(self).prompt_history) + [prompt]
        return f"[Prompt captured for model={self.model or 'unknown'}]"

    # Some helpers (e.g., SQL extraction service) may request structured output.
    def invoke_structured(self, prompt: str, **__: Dict) -> str:  # pragma: no cover - simple stub
        return self.invoke(prompt)


def _capture_prompt(
    question: str,
    agent_type: Optional[str],
    vector_store,
    schema_manager,
    lookml_map,
    k: int,
    gemini_mode: bool,
) -> str:
    """
    Run ``answer_question_simple_gemini`` with a stubbed Gemini client and return the prompt.
    """
    _PromptCaptureClient.prompt_history = []

    patches = [
        patch("simple_rag_simple_gemini.GeminiClient", _PromptCaptureClient),
        patch("llm_registry.GeminiClient", _PromptCaptureClient),
        patch("gemini_client.GeminiClient", _PromptCaptureClient),
        patch("services.sql_extraction_service.GeminiClient", _PromptCaptureClient),
        patch(
            "services.sql_extraction_service.SQLExtractionService.extract_sql",
            lambda self, text, prefer_llm=True, debug=True: self.extract_sql_simple(text, debug),
        ),
    ]

    with patches[0], patches[1], patches[2], patches[3], patches[4]:
        answer_question_simple_gemini(
            question=question,
            vector_store=vector_store,
            k=k,
            gemini_mode=gemini_mode,
            schema_manager=schema_manager,
            lookml_safe_join_map=lookml_map,
            conversation_context="",
            agent_type=agent_type,
            sql_validation=False,
            query_rewriting=False,
            hybrid_search=False,
        )

    history: List[str] = list(_PromptCaptureClient.prompt_history)
    if not history:
        raise RuntimeError("Prompt capture failed ‚Äì ensure FAISS index and schema are available.")
    # Main generation prompt is typically the longest entry in the history.
    return max(history, key=len)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Inspect prompts generated by answer_question_simple_gemini.")
    parser.add_argument(
        "--question",
        required=True,
        help="Natural language question to feed into the pipeline (e.g., 'Top products by revenue').",
    )
    parser.add_argument(
        "--agents",
        nargs="+",
        default=["default", "create"],
        choices=["default", "create", "explain", "longanswer"],
        help="Which agent prompts to capture. 'default' uses the concise chat prompt (no @ flag).",
    )
    parser.add_argument(
        "--index-name",
        default=os.getenv("VECTOR_STORE_NAME") or DEFAULT_VECTOR_STORE,
        help="FAISS index directory name (under rag_app/faiss_indices).",
    )
    parser.add_argument(
        "--k",
        type=int,
        default=6,
        help="Number of retrieved documents to include in the prompt context.",
    )
    parser.add_argument(
        "--gemini-mode",
        action="store_true",
        help="Enable Gemini large-context mode (mirrors Streamlit's Gemini toggle).",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        help="Optional directory to save captured prompts as text files.",
    )
    parser.add_argument(
        "--no-schema",
        action="store_true",
        help="Skip schema manager loading (reduces context, useful if schema CSV is unavailable).",
    )
    parser.add_argument(
        "--no-lookml",
        action="store_true",
        help="Skip LookML safe join map loading.",
    )
    return parser.parse_args()


def ensure_output_dir(path: Optional[Path]) -> Optional[Path]:
    if path is None:
        return None
    path.mkdir(parents=True, exist_ok=True)
    return path


def main() -> None:
    args = parse_args()

    print(f"üîç Inspecting prompts for question: {args.question!r}")
    print(f"üìö Using FAISS index: {args.index_name}")

    vector_store = load_vector_store(args.index_name)
    if vector_store is None:
        raise SystemExit("Failed to load vector store. Ensure embeddings are generated first.")

    schema_manager = None if args.no_schema else load_schema_manager()
    lookml_map = None if args.no_lookml else load_lookml_safe_join_map()

    output_dir = ensure_output_dir(args.output_dir)

    agent_mapping = {
        "default": None,
        "create": "create",
        "explain": "explain",
        "longanswer": "longanswer",
    }

    for agent_label in args.agents:
        agent_type = agent_mapping[agent_label]
        prompt = _capture_prompt(
            question=args.question,
            agent_type=agent_type,
            vector_store=vector_store,
            schema_manager=schema_manager,
            lookml_map=lookml_map,
            k=args.k,
            gemini_mode=args.gemini_mode,
        )

        print("\n" + "=" * 80)
        print(f"Agent: {agent_label}")
        print("=" * 80)
        print(prompt)

        if output_dir:
            filename = output_dir / f"{agent_label}_prompt.txt"
            filename.write_text(prompt, encoding="utf-8")
            print(f"\nüìÑ Saved prompt to {filename}")


if __name__ == "__main__":
    main()
