#!/usr/bin/env python3
"""
GPU-Accelerated Standalone Embedding Generator - Windows Compatible Pre-Build Mode

A high-performance command-line tool for generating embeddings on Windows systems
with NVIDIA GPU acceleration. Uses ThreadPoolExecutor for stable concurrent processing
and leverages Ollama's built-in GPU acceleration and concurrency features.

Optimized for systems with:
- NVIDIA GPUs (RTX A1000 6GB VRAM and similar)
- High RAM (16GB+ recommended)
- Multi-core CPUs

Usage:
    python standalone_embedding_generator.py --csv "queries.csv"
    python standalone_embedding_generator.py --csv "data.csv" --batch-size 300 --workers 16
    python standalone_embedding_generator.py --help

Performance Notes:
- Uses ThreadPoolExecutor (no pickle issues)
- Leverages Ollama GPU acceleration (20-50x faster)
- Supports large batch sizes with high RAM systems
- Optimized for concurrent GPU embedding requests
"""

import os
import sys
import time
import argparse
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
from tqdm import tqdm

# Import required components
try:
    from data_source_manager import DataSourceManager
    from langchain_ollama import OllamaEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_core.documents import Document
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Please ensure all required packages are installed:")
    print("pip install langchain-ollama langchain-community faiss-gpu pandas tqdm")
    print("Note: If faiss-gpu installation fails, fallback to faiss-cpu")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def process_document_batch_gpu(batch_data):
    """
    Full GPU-accelerated worker function for processing a batch of documents
    
    Uses ThreadPoolExecutor (no pickle issues) with both:
    - Ollama GPU acceleration for embeddings
    - FAISS GPU acceleration for vector operations
    
    Args:
        batch_data: Tuple of (doc_data_list, model_name)
        
    Returns:
        FAISS vector store (GPU-enabled if available) or None if failed
    """
    try:
        # Unpack data (no pickle issues with ThreadPoolExecutor)
        doc_data_list, model_name = batch_data
        
        # Import fresh instances (thread-safe)
        from langchain_ollama import OllamaEmbeddings
        from langchain_community.vectorstores import FAISS
        from langchain_core.documents import Document
        
        # Initialize embeddings with GPU acceleration
        # Ollama automatically uses GPU if available and configured
        embeddings = OllamaEmbeddings(model=model_name)
        
        # Recreate Document objects from data
        documents = []
        for doc_data in doc_data_list:
            doc = Document(
                page_content=doc_data['content'],
                metadata=doc_data['metadata']
            )
            documents.append(doc)
        
        # Create vector store with GPU-accelerated embeddings
        vector_store = FAISS.from_documents(documents, embeddings)
        
        # Try to move FAISS index to GPU for faster vector operations
        try:
            import faiss
            if faiss.get_num_gpus() > 0:
                # Move the index to GPU for faster similarity search
                gpu_resources = faiss.StandardGpuResources()
                vector_store.index = faiss.index_cpu_to_gpu(gpu_resources, 0, vector_store.index)
                # print(f"âœ… Moved FAISS index to GPU for batch processing")
        except Exception as gpu_error:
            # Continue with CPU FAISS if GPU fails
            pass
        
        return vector_store
        
    except Exception as e:
        # Safe logging for ThreadPoolExecutor
        print(f"âŒ Error processing batch with GPU acceleration: {e}")
        return None


class GPUStandaloneEmbeddingGenerator:
    """GPU-accelerated standalone embedding generator with Windows-compatible threading"""
    
    def __init__(self, csv_path: str, output_dir: str = "faiss_indices", 
                 batch_size: int = 100, max_workers: int = 16, verbose: bool = False,
                 schema_path: Optional[str] = None):
        """
        Initialize the GPU-accelerated standalone embedding generator
        
        Args:
            csv_path: Path to CSV file with queries
            output_dir: Directory to save vector store
            batch_size: Number of documents per batch (higher values recommended for GPU/high RAM)
            max_workers: Number of concurrent threads (leverage GPU concurrency)
            verbose: Enable detailed logging
            schema_path: Optional path to CSV file with schema information (table_id, column, datatype)
        
        Recommended settings for high-end systems:
            batch_size: 100-300 (for systems with 16GB+ RAM)
            max_workers: 12-20 (for GPU acceleration with multiple cores)
        """
        self.csv_path = Path(csv_path)
        self.output_dir = Path(output_dir)
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.verbose = verbose
        self.schema_path = Path(schema_path) if schema_path else None
        
        # Validate inputs
        if not self.csv_path.exists():
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        # Create output directory
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # Initialize components
        self.embeddings = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=150,
            length_function=len
        )
        
        # Progress tracking
        self.progress_file = self.output_dir / "progress.json"
        self.processed_hashes = set()
        
        if verbose:
            logger.setLevel(logging.DEBUG)
        
        # Check GPU capabilities
        self.gpu_available = self._check_gpu_capabilities()
        
        # Load schema data if provided
        self.schema_lookup = {}
        if self.schema_path:
            try:
                self.schema_lookup = self._load_schema_data(self.schema_path)
                if verbose:
                    schema_tables = len(self.schema_lookup)
                    total_columns = sum(len(cols) for cols in self.schema_lookup.values())
                    print(f"âœ… Loaded schema for {schema_tables} tables ({total_columns} total columns)")
            except Exception as e:
                logger.warning(f"Failed to load schema data: {e}")
                if verbose:
                    print(f"âš ï¸  Schema loading failed: {e}")
                self.schema_lookup = {}
    
    def _check_gpu_capabilities(self) -> Dict[str, bool]:
        """Check GPU capabilities for both FAISS and Ollama"""
        capabilities = {
            'faiss_gpu': False,
            'nvidia_gpu': False,
            'gpu_memory': 0
        }
        
        try:
            # Check FAISS GPU support
            import faiss
            num_gpus = faiss.get_num_gpus()
            capabilities['faiss_gpu'] = num_gpus > 0
            
            if self.verbose and num_gpus > 0:
                print(f"ğŸš€ FAISS GPU support detected: {num_gpus} GPU(s) available")
            elif self.verbose:
                print("âš ï¸  FAISS GPU not available - using CPU mode")
                
        except ImportError:
            if self.verbose:
                print("âš ï¸  faiss-gpu not installed - using faiss-cpu")
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸  FAISS GPU check failed: {e}")
        
        try:
            # Check NVIDIA GPU
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                gpu_memory = int(result.stdout.strip().split('\n')[0])
                capabilities['nvidia_gpu'] = True
                capabilities['gpu_memory'] = gpu_memory
                
                if self.verbose:
                    print(f"ğŸ¯ NVIDIA GPU detected: {gpu_memory} MB VRAM")
                    print("ğŸ’¡ Optimal for RTX A1000 with 6GB VRAM")
            
        except Exception:
            if self.verbose:
                print("âš ï¸  NVIDIA GPU not detected or nvidia-smi unavailable")
        
        return capabilities
    
    def _load_schema_data(self, schema_path: Path) -> Dict[str, List[Tuple[str, str]]]:
        """
        Load schema data from CSV file and create lookup dictionary
        
        Args:
            schema_path: Path to schema CSV file with columns (table_id, column, datatype)
            
        Returns:
            Dictionary mapping table names to list of (column, datatype) tuples
        """
        try:
            if self.verbose:
                print(f"ğŸ“Š Loading schema data from {schema_path}")
            
            schema_df = pd.read_csv(schema_path)
            initial_rows = len(schema_df)
            
            if self.verbose:
                print(f"   ğŸ“„ Read {initial_rows} rows from schema file")
            
            # Validate required columns
            required_cols = ['table_id', 'column', 'datatype']
            missing_cols = [col for col in required_cols if col not in schema_df.columns]
            if missing_cols:
                raise ValueError(f"Schema CSV missing required columns: {missing_cols}")
            
            if self.verbose:
                print(f"   âœ… Schema file has required columns: {required_cols}")
            
            # Remove rows with empty values
            schema_df = schema_df.dropna()
            schema_df = schema_df[
                (schema_df['table_id'].str.strip() != '') &
                (schema_df['column'].str.strip() != '') &
                (schema_df['datatype'].str.strip() != '')
            ]
            
            filtered_rows = len(schema_df)
            if initial_rows != filtered_rows:
                skipped = initial_rows - filtered_rows
                logger.info(f"Filtered out {skipped} rows with empty values")
                if self.verbose:
                    print(f"   ğŸ§¹ Filtered out {skipped} rows with empty values")
            
            # Build lookup dictionary with detailed tracking
            schema_lookup = {}
            processed_tables = set()
            total_columns = 0
            
            for _, row in schema_df.iterrows():
                try:
                    table_id = str(row['table_id']).strip()
                    column = str(row['column']).strip()
                    datatype = str(row['datatype']).strip()
                    
                    # Normalize table name (extract from project.dataset.table format)
                    table_name = self._normalize_table_name(table_id)
                    
                    if table_name not in schema_lookup:
                        schema_lookup[table_name] = []
                        processed_tables.add(table_name)
                    
                    schema_lookup[table_name].append((column, datatype))
                    total_columns += 1
                    
                except Exception as row_error:
                    logger.warning(f"Skipping invalid schema row: {row_error}")
                    continue
            
            # Log detailed statistics
            unique_datatypes = set()
            for table_schema in schema_lookup.values():
                for _, datatype in table_schema:
                    unique_datatypes.add(datatype)
            
            logger.info(f"âœ… Loaded schema for {len(schema_lookup)} tables ({total_columns} columns, {len(unique_datatypes)} data types)")
            
            if self.verbose:
                print(f"   ğŸ“Š Schema Statistics:")
                print(f"      â€¢ Tables: {len(schema_lookup)}")
                print(f"      â€¢ Total Columns: {total_columns}")
                print(f"      â€¢ Data Types: {len(unique_datatypes)}")
                print(f"      â€¢ Top data types: {', '.join(list(unique_datatypes)[:5])}")
                
                # Show sample tables
                sample_tables = list(schema_lookup.keys())[:3]
                if sample_tables:
                    print(f"      â€¢ Sample tables: {', '.join(sample_tables)}")
            
            if not schema_lookup:
                logger.warning("No valid schema data found after processing")
                if self.verbose:
                    print("   âš ï¸  No valid schema data found")
            
            return schema_lookup
            
        except FileNotFoundError:
            error_msg = f"Schema file not found: {schema_path}"
            logger.error(error_msg)
            if self.verbose:
                print(f"   âŒ {error_msg}")
            raise
        except pd.errors.EmptyDataError:
            error_msg = f"Schema file is empty: {schema_path}"
            logger.error(error_msg)
            if self.verbose:
                print(f"   âŒ {error_msg}")
            raise
        except Exception as e:
            error_msg = f"Failed to load schema data from {schema_path}: {e}"
            logger.error(error_msg)
            if self.verbose:
                print(f"   âŒ {error_msg}")
            raise
    
    def _normalize_table_name(self, table_id: str) -> str:
        """
        Extract table name from BigQuery format (project.dataset.table)
        
        Args:
            table_id: Full table identifier (e.g., "project.dataset.customers")
            
        Returns:
            Normalized table name (e.g., "customers")
        """
        # Remove backticks and quotes
        clean_id = table_id.strip('`"\'')
        
        # Extract table name (last part after dots)
        if '.' in clean_id:
            return clean_id.split('.')[-1].strip()
        
        return clean_id.strip()
    
    def _get_table_schema(self, table_name: str) -> List[Tuple[str, str]]:
        """
        Get schema information for a specific table
        
        Args:
            table_name: Name of the table to look up
            
        Returns:
            List of (column, datatype) tuples for the table
        """
        # Normalize the input table name
        normalized_name = self._normalize_table_name(table_name)
        
        # Look up in schema
        return self.schema_lookup.get(normalized_name, [])
    
    def _initialize_embeddings(self) -> bool:
        """Initialize Ollama embeddings with GPU acceleration testing"""
        try:
            if self.verbose:
                print("ğŸ”§ Initializing Ollama GPU-accelerated embeddings...")
            
            self.embeddings = OllamaEmbeddings(model="nomic-embed-text")
            
            # Test connection and GPU acceleration
            start_time = time.time()
            test_result = self.embeddings.embed_query("test connection for GPU acceleration")
            embedding_time = time.time() - start_time
            
            if len(test_result) > 0:
                if self.verbose:
                    print(f"âœ… Ollama connection verified (embedding took {embedding_time:.3f}s)")
                    print(f"ğŸ“Š Embedding dimensions: {len(test_result)}")
                    if embedding_time < 0.1:  # Very fast = likely GPU accelerated
                        print("ğŸš€ GPU acceleration appears to be active!")
                    elif embedding_time < 1.0:  # Fast = good performance
                        print("âš¡ Good embedding performance detected")
                    else:  # Slow = likely CPU only
                        print("âš ï¸  Slower performance - check GPU configuration")
                return True
            else:
                print("âŒ Ollama returned empty embedding")
                return False
                
        except Exception as e:
            print(f"âŒ Failed to initialize Ollama embeddings: {e}")
            print("Make sure Ollama is running with GPU support:")
            print("  1. ollama serve")
            print("  2. ollama pull nomic-embed-text")
            print("  3. Set OLLAMA_NUM_PARALLEL=16 for optimal concurrency")
            return False
    
    def _load_and_validate_csv(self) -> pd.DataFrame:
        """Load and validate CSV data"""
        try:
            df = pd.read_csv(self.csv_path)
            
            # Validate required columns
            required_cols = ['query']
            missing_cols = [col for col in required_cols if col not in df.columns]
            
            if missing_cols:
                raise ValueError(f"Missing required columns: {missing_cols}")
            
            # Check for optional columns and provide defaults
            if 'description' not in df.columns:
                df['description'] = ''
            if 'table' not in df.columns:
                df['table'] = ''
            if 'joins' not in df.columns:
                df['joins'] = ''
            
            # Remove rows with empty queries
            initial_count = len(df)
            df = df[df['query'].notna() & (df['query'].str.strip() != '')]
            final_count = len(df)
            
            if initial_count != final_count:
                print(f"âš ï¸  Removed {initial_count - final_count} rows with empty queries")
            
            if self.verbose:
                print(f"ğŸ“Š Loaded {len(df)} valid queries from CSV")
                print(f"Columns: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            print(f"âŒ Error loading CSV: {e}")
            raise

    def _calculate_csv_hash(self, df: pd.DataFrame) -> str:
        """Calculate hash of CSV content for change detection"""
        try:
            # Create a string representation of the dataframe content
            content_string = ""
            
            # Sort by a stable column if available, otherwise by index
            sorted_df = df.sort_index()
            
            for _, row in sorted_df.iterrows():
                # Create a unique string for each row
                row_parts = []
                for col in ['query', 'description', 'table', 'joins']:
                    if col in row:
                        value = str(row[col]) if pd.notna(row[col]) else ""
                        row_parts.append(f"{col}:{value}")
                
                content_string += "|".join(row_parts) + "\n"
            
            # Return MD5 hash of the content
            return hashlib.md5(content_string.encode('utf-8')).hexdigest()
            
        except Exception as e:
            logger.warning(f"Failed to calculate CSV hash: {e}")
            return ""
    
    def _create_query_fingerprint(self, row: pd.Series) -> str:
        """Create unique fingerprint for a single query including schema information"""
        try:
            # Combine key fields to create unique identifier
            parts = []
            for col in ['query', 'description', 'table', 'joins']:
                if col in row:
                    value = str(row[col]) if pd.notna(row[col]) else ""
                    parts.append(value)
            
            # Include schema information in fingerprint if available
            if self.schema_lookup:
                schema_info = self._get_schema_info_for_query(row)
                if schema_info:
                    # Add schema content to fingerprint
                    schema_content = "|".join(schema_info)
                    parts.append(f"SCHEMA:{schema_content}")
                
                # Also include schema file path modification time if available
                if self.schema_path and self.schema_path.exists():
                    schema_mtime = os.path.getmtime(self.schema_path)
                    parts.append(f"SCHEMA_MTIME:{schema_mtime}")
            
            content = "|".join(parts)
            return hashlib.md5(content.encode('utf-8')).hexdigest()
            
        except Exception as e:
            logger.warning(f"Failed to create query fingerprint: {e}")
            return ""
    
    def _load_processing_status(self) -> Dict:
        """Load detailed processing status with incremental tracking"""
        csv_name = self.csv_path.stem
        status_file = self.output_dir / f"status_{csv_name}.json"
        
        if not status_file.exists():
            return {
                'csv_file': str(self.csv_path),
                'csv_name': csv_name,
                'csv_hash': '',
                'csv_modification_time': '',
                'processed_query_fingerprints': [],
                'total_documents': 0,
                'last_processed': '',
                'processing_history': []
            }
        
        try:
            import json
            with open(status_file, 'r') as f:
                status = json.load(f)
            
            # Ensure all required fields exist for backward compatibility
            if 'processed_query_fingerprints' not in status:
                status['processed_query_fingerprints'] = []
            if 'csv_hash' not in status:
                status['csv_hash'] = ''
            if 'csv_modification_time' not in status:
                status['csv_modification_time'] = ''
            if 'processing_history' not in status:
                status['processing_history'] = []
            
            return status
            
        except Exception as e:
            logger.warning(f"Failed to load processing status: {e}")
            # Return empty status instead of recursion
            return {
                'csv_file': str(self.csv_path),
                'csv_name': self.csv_path.stem,
                'csv_hash': '',
                'csv_modification_time': '',
                'processed_query_fingerprints': [],
                'total_documents': 0,
                'last_processed': '',
                'processing_history': []
            }
    
    def _identify_new_queries(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """
        Identify new queries that haven't been processed yet
        
        Returns:
            Tuple of (new_queries_df, new_fingerprints_list)
        """
        status = self._load_processing_status()
        processed_fingerprints = set(status.get('processed_query_fingerprints', []))
        
        new_rows = []
        new_fingerprints = []
        
        for idx, row in df.iterrows():
            fingerprint = self._create_query_fingerprint(row)
            
            if fingerprint not in processed_fingerprints:
                new_rows.append(row)
                new_fingerprints.append(fingerprint)
        
        if new_rows:
            new_df = pd.DataFrame(new_rows)
            new_df.reset_index(drop=True, inplace=True)
        else:
            new_df = pd.DataFrame()
        
        return new_df, new_fingerprints
    
    def _has_csv_changed(self, df: pd.DataFrame) -> Tuple[bool, str]:
        """
        Check if CSV content has changed since last processing
        
        Returns:
            Tuple of (has_changed, reason)
        """
        try:
            status = self._load_processing_status()
            current_hash = self._calculate_csv_hash(df)
            stored_hash = status.get('csv_hash', '')
            
            # Get file modification time
            csv_mod_time = os.path.getmtime(self.csv_path)
            csv_mod_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(csv_mod_time))
            stored_mod_time = status.get('csv_modification_time', '')
            
            if not stored_hash:
                return True, "No previous processing found"
            
            if current_hash != stored_hash:
                return True, f"CSV content changed (hash: {stored_hash[:8]} â†’ {current_hash[:8]})"
            
            if csv_mod_str != stored_mod_time:
                return True, f"CSV file modified (time: {stored_mod_time} â†’ {csv_mod_str})"
            
            return False, "No changes detected"
            
        except Exception as e:
            logger.warning(f"Error checking CSV changes: {e}")
            return True, "Error during change detection"
    
    def _create_documents(self, df: pd.DataFrame) -> List[Document]:
        """Create clean LangChain documents from DataFrame WITHOUT schema (for better semantic matching)"""
        documents = []
        
        # Track schema coverage statistics
        total_queries = len(df)
        queries_with_schema = 0
        total_tables_found = 0
        total_tables_with_schema = 0
        
        if self.verbose:
            if self.schema_lookup:
                print(f"âš¡ Creating clean documents (schema-free) for {total_queries} queries...")
                print(f"ğŸ—ƒï¸  Schema available for dynamic injection: {self.table_count:,} tables")
            else:
                print(f"ğŸ“Š Creating documents for {total_queries} queries...")
        
        for idx, row in df.iterrows():
            # Create composite content for richer search
            content_parts = [f"Query: {row['query']}"]
            
            if row.get('description') and str(row['description']).strip():
                content_parts.append(f"Description: {row['description']}")
            
            if row.get('table') and str(row['table']).strip():
                content_parts.append(f"Tables: {row['table']}")
            
            if row.get('joins') and str(row['joins']).strip():
                content_parts.append(f"Joins: {row['joins']}")
            
            # REMOVED: Schema information to reduce noise in embeddings
            # Schema will be injected dynamically during query time via SchemaManager
            # This creates cleaner embeddings for better semantic matching
            
            # Track table coverage for statistics
            if self.schema_lookup:
                query_tables = self._extract_tables_from_row(row)
                total_tables_found += len(query_tables)
                for table in query_tables:
                    if self._get_table_schema(table):
                        total_tables_with_schema += 1
            
            content = "\n".join(content_parts)
            
            # Create document with metadata (including schema info)
            metadata = {
                'index': idx,
                'query': row['query'],
                'description': str(row.get('description', '')),
                'table': str(row.get('table', '')),
                'joins': str(row.get('joins', '')),
                'source': self.csv_path.name
            }
            
            # Note: Schema metadata removed to keep embeddings clean
            # Schema will be available via SchemaManager during query time
            metadata['has_schema'] = False
            metadata['schema_tables'] = []
            
            doc = Document(page_content=content, metadata=metadata)
            documents.append(doc)
        
        # Log schema availability statistics (schema not included in embeddings)
        if self.schema_lookup and self.verbose:
            table_coverage = (total_tables_with_schema / total_tables_found * 100) if total_tables_found > 0 else 0
            
            logger.info(f"ğŸ“Š Schema Available for Dynamic Injection:")
            logger.info(f"   â€¢ Total tables in schema: {self.table_count:,}")
            logger.info(f"   â€¢ Tables found in queries: {total_tables_found}")
            logger.info(f"   â€¢ Tables with schema available: {total_tables_with_schema}/{total_tables_found} ({table_coverage:.1f}%)")
            
            print(f"   âš¡ Clean Embeddings: Schema removed from embeddings for better semantic matching")
            print(f"   ğŸ—ƒï¸  Schema Available: {self.table_count:,} tables ready for dynamic injection via SchemaManager")
        
        return documents
    
    def _extract_tables_from_row(self, row: pd.Series) -> List[str]:
        """
        Extract table names from a query row's metadata
        
        Args:
            row: DataFrame row containing query metadata
            
        Returns:
            List of normalized table names
        """
        tables = []
        
        # Get tables from the 'table' or 'tables' column
        table_data = row.get('table') or row.get('tables', '')
        
        if table_data and str(table_data).strip():
            table_str = str(table_data).strip()
            
            # Try to parse as list first (if it's a string representation of a list)
            try:
                import ast
                if table_str.startswith('[') and table_str.endswith(']'):
                    parsed_tables = ast.literal_eval(table_str)
                    if isinstance(parsed_tables, list):
                        tables = [self._normalize_table_name(str(t)) for t in parsed_tables if str(t).strip()]
                    else:
                        tables = [self._normalize_table_name(table_str)]
                else:
                    # Parse as comma-separated string
                    if ',' in table_str:
                        tables = [self._normalize_table_name(t.strip()) for t in table_str.split(',') if t.strip()]
                    else:
                        tables = [self._normalize_table_name(table_str)]
            except:
                # Fallback to simple parsing
                if ',' in table_str:
                    tables = [self._normalize_table_name(t.strip()) for t in table_str.split(',') if t.strip()]
                else:
                    tables = [self._normalize_table_name(table_str)]
        
        # Remove empty strings and duplicates while preserving order
        unique_tables = []
        for table in tables:
            if table and table not in unique_tables:
                unique_tables.append(table)
        
        return unique_tables
    
    def _get_schema_info_for_query(self, row: pd.Series) -> List[str]:
        """
        Generate schema information content for a query row
        
        Args:
            row: DataFrame row containing query metadata
            
        Returns:
            List of content strings with schema information
        """
        if not self.schema_lookup:
            return []
        
        try:
            # Extract tables from the row
            tables = self._extract_tables_from_row(row)
            
            if not tables:
                return []
            
            schema_content = []
            table_schemas = {}
            all_columns = []
            tables_with_schema = []
            tables_without_schema = []
            
            # Gather schema info for each table with tracking
            for table in tables:
                try:
                    schema = self._get_table_schema(table)
                    if schema:
                        table_schemas[table] = schema
                        all_columns.extend([(table, col, dtype) for col, dtype in schema])
                        tables_with_schema.append(table)
                    else:
                        tables_without_schema.append(table)
                except Exception as table_error:
                    logger.debug(f"Error getting schema for table '{table}': {table_error}")
                    tables_without_schema.append(table)
            
            # Log schema coverage if verbose
            if self.verbose and (tables_with_schema or tables_without_schema):
                coverage = len(tables_with_schema) / len(tables) * 100 if tables else 0
                logger.debug(f"Schema coverage for query: {coverage:.1f}% ({len(tables_with_schema)}/{len(tables)} tables)")
                if tables_without_schema:
                    logger.debug(f"Tables without schema: {', '.join(tables_without_schema)}")
            
            if not table_schemas:
                return []
            
            # Format schema information for embedding
            schema_lines = []
            type_mappings = []
            
            for table, schema in table_schemas.items():
                if schema:
                    try:
                        # Create detailed schema line
                        columns_with_types = [f"{col} ({dtype})" for col, dtype in schema]
                        schema_lines.append(f"- {table}: {', '.join(columns_with_types)}")
                        
                        # Create type mappings for searchability
                        for col, dtype in schema:
                            type_mappings.append(f"{col}â†’{dtype}")
                    except Exception as format_error:
                        logger.warning(f"Error formatting schema for table '{table}': {format_error}")
                        continue
            
            if schema_lines:
                schema_content.append("Schema Context:")
                schema_content.extend(schema_lines)
                
                # Add searchable column type information
                if type_mappings:
                    schema_content.append(f"Column Types: {', '.join(type_mappings)}")
                
                # Add data type summary for search
                datatypes = list(set(dtype for _, _, dtype in all_columns))
                if datatypes:
                    schema_content.append(f"Data Types Used: {', '.join(sorted(datatypes))}")
                
                # Add schema coverage information for search
                if tables_without_schema:
                    schema_content.append(f"Partial Schema Coverage: {len(tables_with_schema)}/{len(tables)} tables")
            
            return schema_content
            
        except Exception as e:
            logger.warning(f"Error generating schema info for query: {e}")
            return []
    
    def _split_documents(self, documents: List[Document]) -> List[Document]:
        """Split documents into chunks if needed"""
        if self.verbose:
            print("ğŸ“„ Splitting documents into chunks...")
        
        split_docs = self.text_splitter.split_documents(documents)
        
        if self.verbose:
            print(f"Created {len(split_docs)} chunks from {len(documents)} documents")
        
        return split_docs
    
    def _calculate_batch_hash(self, docs_batch: List[Document]) -> str:
        """Calculate hash for a batch of documents"""
        content = "\n".join([doc.page_content for doc in docs_batch])
        return hashlib.md5(content.encode()).hexdigest()
    
    def _load_progress(self) -> Dict:
        """Load processing progress from file"""
        if self.progress_file.exists():
            try:
                import json
                with open(self.progress_file, 'r') as f:
                    progress = json.load(f)
                    self.processed_hashes = set(progress.get('processed_hashes', []))
                    return progress
            except Exception as e:
                if self.verbose:
                    print(f"âš ï¸  Could not load progress file: {e}")
        
        return {'processed_hashes': [], 'completed_batches': 0, 'total_batches': 0}
    
    def _save_progress(self, progress: Dict):
        """Save processing progress to file"""
        try:
            import json
            progress['processed_hashes'] = list(self.processed_hashes)
            with open(self.progress_file, 'w') as f:
                json.dump(progress, f)
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸  Could not save progress: {e}")
    
    def _prepare_batch_data(self, docs_batch: List[Document]) -> Tuple[List[Dict], str]:
        """Convert Document objects to serializable data for Windows multiprocessing"""
        doc_data_list = []
        for doc in docs_batch:
            doc_data = {
                'content': doc.page_content,
                'metadata': dict(doc.metadata)  # Ensure metadata is serializable
            }
            doc_data_list.append(doc_data)
        
        return doc_data_list, "nomic-embed-text"
    
    def _generate_embeddings_gpu_parallel(self, documents: List[Document], resume: bool = False) -> FAISS:
        """Generate embeddings using GPU-accelerated ThreadPoolExecutor (Windows-compatible)"""
        
        # Load progress if resuming
        progress = self._load_progress() if resume else {'processed_hashes': [], 'completed_batches': 0, 'total_batches': 0}
        
        # Split into batches
        batches = [documents[i:i + self.batch_size] for i in range(0, len(documents), self.batch_size)]
        total_batches = len(batches)
        progress['total_batches'] = total_batches
        
        print(f"ğŸ”§ Processing {len(documents)} documents in {total_batches} batches")
        print(f"ğŸ“Š Batch size: {self.batch_size}, GPU-accelerated workers: {self.max_workers}")
        print("ğŸš€ Using ThreadPoolExecutor with Ollama GPU acceleration")
        
        if resume and progress['completed_batches'] > 0:
            print(f"ğŸ“ Resuming from batch {progress['completed_batches']}/{total_batches}")
        
        # Process batches
        vector_stores = []
        start_time = time.time()
        
        # Filter out already processed batches if resuming
        remaining_batches = batches
        if resume:
            remaining_batches = batches[progress['completed_batches']:]
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Prepare batch data for GPU-accelerated processing
            batch_data_list = []
            for batch in remaining_batches:
                batch_data = self._prepare_batch_data(batch)
                batch_data_list.append(batch_data)
            
            # Submit all remaining batches to GPU-accelerated workers
            future_to_batch = {
                executor.submit(process_document_batch_gpu, batch_data): (i, batch_data) 
                for i, batch_data in enumerate(batch_data_list)
            }
            
            # Process completed batches
            with tqdm(total=len(remaining_batches), desc="Processing batches", unit="batch") as pbar:
                for future in as_completed(future_to_batch):
                    batch_idx, batch_data = future_to_batch[future]
                    
                    try:
                        vector_store = future.result()
                        if vector_store:
                            vector_stores.append(vector_store)
                            
                            # Update progress (calculate hash from batch data)
                            doc_data_list, _ = batch_data
                            content = "\n".join([doc_data['content'] for doc_data in doc_data_list])
                            batch_hash = hashlib.md5(content.encode()).hexdigest()
                            self.processed_hashes.add(batch_hash)
                            progress['completed_batches'] += 1
                            
                            # Save progress periodically
                            if progress['completed_batches'] % 5 == 0:
                                self._save_progress(progress)
                            
                            # Update progress bar with timing info
                            elapsed = time.time() - start_time
                            if progress['completed_batches'] > 0:
                                avg_time = elapsed / progress['completed_batches']
                                remaining = total_batches - progress['completed_batches']
                                eta = avg_time * remaining
                                pbar.set_postfix({
                                    'ETA': f"{eta/60:.1f}m",
                                    'Completed': f"{progress['completed_batches']}/{total_batches}"
                                })
                            
                        else:
                            print(f"âŒ Failed to process batch {batch_idx + 1}")
                    
                    except Exception as e:
                        print(f"âŒ Error in batch {batch_idx + 1}: {e}")
                    
                    pbar.update(1)
        
        # Save final progress
        self._save_progress(progress)
        
        if not vector_stores:
            raise RuntimeError("No vector stores were created successfully")
        
        # Merge all vector stores with GPU optimization
        print("ğŸ”— Merging vector stores...")
        main_store = vector_stores[0]
        
        # Merge stores
        for store in vector_stores[1:]:
            main_store.merge_from(store)
        
        # Final GPU optimization for the merged store
        try:
            import faiss
            if self.gpu_available.get('faiss_gpu', False):
                print("ğŸš€ Optimizing final vector store with GPU acceleration...")
                gpu_resources = faiss.StandardGpuResources()
                main_store.index = faiss.index_cpu_to_gpu(gpu_resources, 0, main_store.index)
                print(f"âœ… Final vector store moved to GPU ({self.gpu_available['gpu_memory']} MB VRAM)")
        except Exception as e:
            print(f"âš ï¸  GPU optimization failed, using CPU mode: {e}")
        
        processing_time = time.time() - start_time
        print(f"âœ… Completed in {processing_time/60:.1f} minutes")
        
        # Report GPU utilization
        if self.gpu_available.get('faiss_gpu') and self.gpu_available.get('nvidia_gpu'):
            estimated_vram = len(main_store.docstore._dict) * 0.5  # Rough estimate
            print(f"ğŸ“Š GPU Performance Summary:")
            print(f"   ğŸ¯ NVIDIA GPU: {self.gpu_available['gpu_memory']} MB total VRAM")
            print(f"   ğŸ“¦ Estimated FAISS usage: ~{estimated_vram:.0f} MB")
            print(f"   âš¡ Both embeddings and vector ops GPU-accelerated")
        
        return main_store
    
    def _save_vector_store(self, vector_store: FAISS, csv_name: str, df: pd.DataFrame = None, 
                          new_fingerprints: List[str] = None, is_incremental: bool = False):
        """Save vector store to disk with enhanced incremental tracking"""
        index_path = self.output_dir / f"index_{csv_name}"
        
        try:
            # Convert GPU index to CPU before saving (required for serialization)
            gpu_accelerated = False
            try:
                import faiss
                if hasattr(vector_store.index, 'device') or 'Gpu' in str(type(vector_store.index)):
                    # Convert GPU index to CPU for saving
                    vector_store.index = faiss.index_gpu_to_cpu(vector_store.index)
                    gpu_accelerated = True
                    if self.verbose:
                        print("ğŸ”„ Converted GPU index to CPU for disk storage")
            except Exception as e:
                if self.verbose:
                    print(f"âš ï¸  GPU to CPU conversion warning: {e}")
            
            vector_store.save_local(str(index_path))
            
            # Load existing status for incremental updates
            status = self._load_processing_status()
            
            # Update status with new information
            current_time = time.strftime('%Y-%m-%d %H:%M:%S')
            
            status.update({
                'csv_file': str(self.csv_path),
                'csv_name': csv_name,
                'total_documents': len(vector_store.docstore._dict),
                'last_processed': current_time,
                'batch_size': self.batch_size,
                'max_workers': self.max_workers,
                'gpu_acceleration': {
                    'faiss_gpu_available': self.gpu_available.get('faiss_gpu', False),
                    'nvidia_gpu_detected': self.gpu_available.get('nvidia_gpu', False),
                    'gpu_memory_mb': self.gpu_available.get('gpu_memory', 0),
                    'gpu_accelerated_processing': gpu_accelerated
                }
            })
            
            # Update incremental tracking data
            if df is not None:
                # Update CSV hash and modification time
                status['csv_hash'] = self._calculate_csv_hash(df)
                csv_mod_time = os.path.getmtime(self.csv_path)
                status['csv_modification_time'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(csv_mod_time))
                
                if is_incremental and new_fingerprints:
                    # Add new fingerprints to processed list
                    existing_fingerprints = set(status.get('processed_query_fingerprints', []))
                    existing_fingerprints.update(new_fingerprints)
                    status['processed_query_fingerprints'] = list(existing_fingerprints)
                    
                    # Add to processing history
                    if 'processing_history' not in status:
                        status['processing_history'] = []
                    
                    status['processing_history'].append({
                        'timestamp': current_time,
                        'type': 'incremental',
                        'new_queries_added': len(new_fingerprints),
                        'total_queries_after': len(existing_fingerprints)
                    })
                else:
                    # Full rebuild - create all fingerprints
                    all_fingerprints = []
                    for _, row in df.iterrows():
                        fingerprint = self._create_query_fingerprint(row)
                        if fingerprint:
                            all_fingerprints.append(fingerprint)
                    
                    status['processed_query_fingerprints'] = all_fingerprints
                    
                    # Add to processing history
                    if 'processing_history' not in status:
                        status['processing_history'] = []
                    
                    status['processing_history'].append({
                        'timestamp': current_time,
                        'type': 'full_rebuild',
                        'total_queries': len(all_fingerprints)
                    })
            
            # Maintain backward compatibility
            if 'created_at' not in status:
                status['created_at'] = current_time
            
            import json
            status_file = self.output_dir / f"status_{csv_name}.json"
            with open(status_file, 'w') as f:
                json.dump(status, f, indent=2)
            
            print(f"ğŸ“ Vector store saved to: {index_path}")
            print(f"ğŸ“„ Status saved to: {status_file}")
            
            if is_incremental and new_fingerprints:
                print(f"ğŸ”„ Incremental update: Added {len(new_fingerprints)} new queries")
            
        except Exception as e:
            print(f"âŒ Error saving vector store: {e}")
            raise
    
    def generate(self, resume: bool = False, force_rebuild: bool = False, incremental: bool = False) -> bool:
        """
        Generate embeddings from CSV file with incremental update support
        
        Args:
            resume: Resume interrupted processing
            force_rebuild: Rebuild even if vector store exists
            incremental: Only process new queries (detect changes)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            csv_name = self.csv_path.stem
            index_path = self.output_dir / f"index_{csv_name}"
            
            # Load and validate CSV first (needed for all modes)
            df = self._load_and_validate_csv()
            print(f"ğŸ“Š Loaded {len(df)} queries from CSV")
            
            # Incremental mode logic
            if incremental and index_path.exists() and not force_rebuild:
                print("ğŸ” Incremental mode: Checking for changes...")
                
                has_changed, reason = self._has_csv_changed(df)
                
                if not has_changed:
                    print(f"âœ… No changes detected: {reason}")
                    print("Vector store is up to date!")
                    return True
                
                print(f"ğŸ“ Changes detected: {reason}")
                
                # Identify new queries
                new_df, new_fingerprints = self._identify_new_queries(df)
                
                if new_df.empty:
                    print("â„¹ï¸  File changed but no new queries found (modifications to existing queries)")
                    print("ğŸ’¡ Use --force-rebuild to rebuild with modified queries")
                    return True
                
                print(f"ğŸ†• Found {len(new_df)} new queries to process")
                
                # Load existing vector store
                print("ğŸ“‚ Loading existing vector store...")
                existing_vector_store = self._load_existing_vector_store(index_path)
                if not existing_vector_store:
                    print("âš ï¸  Could not load existing vector store, falling back to full rebuild")
                    force_rebuild = True
                    incremental = False
                else:
                    # Process only new queries
                    print("ğŸ”„ Processing new queries with GPU acceleration...")
                    new_documents = self._create_documents(new_df)
                    new_split_documents = self._split_documents(new_documents)
                    
                    # Generate embeddings for new documents
                    new_vector_store = self._generate_embeddings_gpu_parallel(new_split_documents, resume=False)
                    
                    # Merge with existing vector store
                    print("ğŸ”— Merging with existing vector store...")
                    existing_vector_store.merge_from(new_vector_store)
                    
                    # Save merged vector store with incremental tracking
                    self._save_vector_store(existing_vector_store, csv_name, df, new_fingerprints, is_incremental=True)
                    
                    print("âœ… Incremental update completed successfully!")
                    print(f"ğŸ“Š Added {len(new_df)} new queries to existing {len(existing_vector_store.docstore._dict) - len(new_df)} queries")
                    print("ğŸš€ You can now run 'streamlit run app.py'")
                    
                    return True
            
            # Standard processing (full rebuild or first time)
            if not incremental and index_path.exists() and not force_rebuild and not resume:
                print(f"âœ… Vector store already exists: {index_path}")
                print("Use --force-rebuild to rebuild, --resume to continue interrupted processing, or --incremental for smart updates")
                return True
            
            # Report processing mode
            if force_rebuild:
                print("ğŸ”„ Force rebuild mode: Recreating entire vector store...")
            elif resume:
                print("â–¶ï¸  Resume mode: Continuing interrupted processing...")
            else:
                print("ğŸ”„ GPU-accelerated embedding generation starting...")
            
            print("ğŸš€ Leveraging Ollama GPU acceleration with ThreadPoolExecutor")
            
            # Report GPU capabilities
            if self.gpu_available.get('faiss_gpu') and self.gpu_available.get('nvidia_gpu'):
                print(f"ğŸ¯ Full GPU acceleration enabled:")
                print(f"   ğŸ“Š NVIDIA GPU: {self.gpu_available['gpu_memory']} MB VRAM")
                print(f"   âš¡ Ollama embeddings: GPU-accelerated")
                print(f"   ğŸš€ FAISS vector operations: GPU-accelerated")
            elif self.gpu_available.get('nvidia_gpu'):
                print(f"âš¡ Partial GPU acceleration:")
                print(f"   ğŸ“Š NVIDIA GPU: {self.gpu_available['gpu_memory']} MB VRAM")
                print(f"   âš¡ Ollama embeddings: GPU-accelerated")
                print(f"   âš ï¸  FAISS operations: CPU mode (install faiss-gpu)")
            else:
                print("âš ï¸  CPU-only mode detected")
                print("ğŸ’¡ Consider installing faiss-gpu and ensuring NVIDIA GPU drivers")
            
            # Initialize Ollama
            if not self._initialize_embeddings():
                return False
            
            # Create documents
            documents = self._create_documents(df)
            split_documents = self._split_documents(documents)
            
            # Generate embeddings with GPU-accelerated threading
            vector_store = self._generate_embeddings_gpu_parallel(split_documents, resume=resume)
            
            # Save vector store with full tracking
            self._save_vector_store(vector_store, csv_name, df, is_incremental=False)
            
            # Clean up progress file on successful completion
            if self.progress_file.exists():
                self.progress_file.unlink()
            
            print("âœ… All embeddings generated successfully!")
            print(f"â±ï¸  Total documents processed: {len(vector_store.docstore._dict)}")
            print("ğŸš€ You can now run 'streamlit run app.py'")
            
            return True
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Processing interrupted by user")
            print("Use --resume flag to continue from where you left off")
            return False
        except Exception as e:
            print(f"âŒ Error during generation: {e}")
            if self.verbose:
                import traceback
                traceback.print_exc()
            return False
    
    def _load_existing_vector_store(self, index_path) -> Optional:
        """Load existing FAISS vector store from disk"""
        try:
            from langchain_ollama import OllamaEmbeddings
            from langchain_community.vectorstores import FAISS
            
            embeddings = OllamaEmbeddings(model="nomic-embed-text")
            vector_store = FAISS.load_local(
                str(index_path),
                embeddings,
                allow_dangerous_deserialization=True
            )
            
            print(f"âœ… Loaded existing vector store with {len(vector_store.docstore._dict)} documents")
            return vector_store
            
        except Exception as e:
            print(f"âŒ Error loading existing vector store: {e}")
            return None


def main():
    """Main entry point for GPU-accelerated standalone embedding generator"""
    parser = argparse.ArgumentParser(
        description="GPU-accelerated standalone embedding generator with Windows compatibility",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage (optimal for most systems)
  python standalone_embedding_generator.py --csv "queries.csv"
  
  # Schema-enhanced mode (recommended for better search accuracy)
  python standalone_embedding_generator.py --csv "queries.csv" --schema "table_schemas.csv"
  
  # High-performance settings for powerful systems (RTX A1000+ with 32GB+ RAM)
  python standalone_embedding_generator.py --csv "data.csv" --batch-size 300 --workers 16
  
  # Schema-enhanced with high performance
  python standalone_embedding_generator.py --csv "data.csv" --schema "schemas.csv" --batch-size 300 --workers 16
  
  # Moderate settings for mid-range systems
  python standalone_embedding_generator.py --csv "data.csv" --batch-size 150 --workers 8
  
  # Resume interrupted processing (with schema)
  python standalone_embedding_generator.py --csv "data.csv" --schema "schemas.csv" --resume
  
  # Force rebuild existing store
  python standalone_embedding_generator.py --csv "data.csv" --force-rebuild
  
  # Incremental updates (smart - only process new queries)
  python standalone_embedding_generator.py --csv "data.csv" --incremental
  
Performance Notes:
  - GPU acceleration provides 20-50x speedup over CPU-only processing
  - Incremental mode processes only new queries (seconds vs minutes)
  - Higher batch sizes recommended for systems with 16GB+ RAM
  - More workers leverage GPU concurrency (recommend 12-20 for RTX A1000)
  - Set OLLAMA_NUM_PARALLEL=16 environment variable for optimal GPU utilization
        """
    )
    
    parser.add_argument(
        '--csv', required=True,
        help='Path to CSV file containing queries'
    )
    
    parser.add_argument(
        '--output', default='faiss_indices',
        help='Output directory for vector store (default: faiss_indices)'
    )
    
    parser.add_argument(
        '--schema',
        help='Path to CSV file containing table schema information (table_id, column, datatype)'
    )
    
    parser.add_argument(
        '--batch-size', type=int, default=100,
        help='Documents per batch (higher values recommended for GPU/high RAM systems, default: 100)'
    )
    
    parser.add_argument(
        '--workers', type=int, default=16,
        help='Number of concurrent threads (leverage GPU concurrency, default: 16)'
    )
    
    parser.add_argument(
        '--resume', action='store_true',
        help='Resume interrupted processing'
    )
    
    parser.add_argument(
        '--force-rebuild', action='store_true',
        help='Rebuild even if vector store exists'
    )
    
    parser.add_argument(
        '--incremental', action='store_true',
        help='Smart incremental updates - only process new queries'
    )
    
    parser.add_argument(
        '--verbose', action='store_true',
        help='Enable detailed output'
    )
    
    args = parser.parse_args()
    
    # Validate arguments
    if not Path(args.csv).exists():
        print(f"âŒ CSV file not found: {args.csv}")
        return 1
    
    # Validate schema file if provided
    if args.schema:
        schema_path = Path(args.schema)
        if not schema_path.exists():
            print(f"âŒ Schema file not found: {args.schema}")
            return 1
        
        # Validate schema file format
        try:
            import pandas as pd
            schema_df = pd.read_csv(schema_path, nrows=5)  # Just read first few rows for validation
            required_cols = ['table_id', 'column', 'datatype']
            missing_cols = [col for col in required_cols if col not in schema_df.columns]
            if missing_cols:
                print(f"âŒ Schema file missing required columns: {missing_cols}")
                print(f"   Expected columns: {required_cols}")
                print(f"   Found columns: {list(schema_df.columns)}")
                return 1
        except Exception as e:
            print(f"âŒ Schema file validation failed: {e}")
            print(f"   Please ensure {args.schema} is a valid CSV file with columns: table_id, column, datatype")
            return 1
    
    if args.batch_size < 1:
        print("âŒ Batch size must be at least 1")
        return 1
    
    if args.workers < 1:
        print("âŒ Number of workers must be at least 1")
        return 1
    
    # Performance optimization suggestions
    if args.workers > 20:
        print("âš ï¸  Very high worker count (>20) may not provide additional benefits")
    if args.batch_size > 500:
        print("âš ï¸  Very large batch size (>500) may cause memory issues on some systems")
    
    # GPU acceleration reminder
    print("ğŸ’¡ For optimal GPU performance, ensure:")
    print("   1. Ollama is running: ollama serve")
    print("   2. GPU model loaded: ollama pull nomic-embed-text") 
    print("   3. Set concurrency: export OLLAMA_NUM_PARALLEL=16")
    
    # Create generator
    try:
        generator = GPUStandaloneEmbeddingGenerator(
            csv_path=args.csv,
            output_dir=args.output,
            batch_size=args.batch_size,
            max_workers=args.workers,
            verbose=args.verbose,
            schema_path=args.schema
        )
        
        # Generate embeddings
        success = generator.generate(
            resume=args.resume,
            force_rebuild=args.force_rebuild,
            incremental=args.incremental
        )
        
        return 0 if success else 1
        
    except Exception as e:
        print(f"âŒ Fatal error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    # GPU-accelerated threading approach - no multiprocessing setup needed
    # ThreadPoolExecutor is Windows-compatible and avoids pickle issues
    print("ğŸš€ Starting GPU-accelerated embedding generation...")
    sys.exit(main())