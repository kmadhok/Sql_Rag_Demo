{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Debug & Analysis Notebook\n",
    "\n",
    "This notebook provides comprehensive debugging and analysis tools for your SQL RAG system. \n",
    "Use this to understand:\n",
    "\n",
    "- **What embeddings are being generated**\n",
    "- **Which documents are retrieved and why**\n",
    "- **What schema context is being injected**\n",
    "- **What exactly is being sent to the LLM**\n",
    "- **Why certain queries fail to find relevant tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add current directory to path\n",
    "current_dir = Path.cwd()\n",
    "if str(current_dir) not in sys.path:\n",
    "    sys.path.append(str(current_dir))\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# Local imports\n",
    "from schema_manager import SchemaManager\n",
    "from actions.llm_interaction import generate_answer_from_context, initialize_llm_client\n",
    "\n",
    "# Visualization imports\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è sklearn not available - install with: pip install scikit-learn\")\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è umap not available - install with: pip install umap-learn\")\n",
    "    UMAP_AVAILABLE = False\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"üìÅ Current directory: {current_dir}\")\n",
    "print(f\"üî¨ Visualization tools: sklearn={SKLEARN_AVAILABLE}, umap={UMAP_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available vector stores\n",
    "faiss_dir = current_dir / \"faiss_indices\"\n",
    "\n",
    "if not faiss_dir.exists():\n",
    "    print(\"‚ùå No faiss_indices directory found\")\n",
    "    print(\"Please run your embedding generation script first\")\n",
    "else:\n",
    "    indices = [d for d in faiss_dir.iterdir() if d.is_dir() and d.name.startswith(\"index_\")]\n",
    "    \n",
    "    print(f\"üì¶ Found {len(indices)} vector stores:\")\n",
    "    for i, idx in enumerate(indices):\n",
    "        index_file = idx / \"index.faiss\"\n",
    "        pkl_file = idx / \"index.pkl\"\n",
    "        \n",
    "        if index_file.exists() and pkl_file.exists():\n",
    "            size_mb = (index_file.stat().st_size + pkl_file.stat().st_size) / (1024*1024)\n",
    "            print(f\"  {i+1}. {idx.name} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"  {i+1}. {idx.name} (INCOMPLETE - missing files)\")\n",
    "    \n",
    "    # Store for later use\n",
    "    available_indices = indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Embedding Model and Load Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "print(\"üîÑ Initializing embedding model...\")\n",
    "try:\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    \n",
    "    # Test embedding generation\n",
    "    test_embedding = embeddings.embed_query(\"test query\")\n",
    "    print(f\"‚úÖ Embedding model loaded: {len(test_embedding)}-dimensional vectors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load embedding model: {e}\")\n",
    "    print(\"Make sure Ollama is running and nomic-embed-text model is available\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and load a vector store\n",
    "if available_indices and embeddings:\n",
    "    # Use the first available index (change this as needed)\n",
    "    selected_index = available_indices[0]\n",
    "    print(f\"üîÑ Loading vector store: {selected_index.name}\")\n",
    "    \n",
    "    try:\n",
    "        vector_store = FAISS.load_local(\n",
    "            str(selected_index),\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        \n",
    "        # Get basic info about the vector store\n",
    "        num_docs = len(vector_store.docstore._dict)\n",
    "        print(f\"‚úÖ Vector store loaded: {num_docs} documents\")\n",
    "        \n",
    "        # Sample some documents\n",
    "        sample_docs = vector_store.similarity_search(\"sample query\", k=3)\n",
    "        print(f\"üìä Sample documents loaded: {len(sample_docs)} examples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load vector store: {e}\")\n",
    "        vector_store = None\n",
    "else:\n",
    "    vector_store = None\n",
    "    print(\"‚ùå No vector store available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Schema Manager Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for schema files\n",
    "schema_files = list(current_dir.glob(\"*.csv\"))\n",
    "schema_files.extend(list(current_dir.glob(\"*schema*.csv\")))\n",
    "schema_files.extend(list(current_dir.glob(\"../retail_system/**/*.csv\")))\n",
    "\n",
    "print(f\"üìã Found {len(schema_files)} potential schema files:\")\n",
    "for f in schema_files[:5]:  # Show first 5\n",
    "    print(f\"  - {f.name} ({f.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "# Initialize schema manager if files found\n",
    "schema_manager = None\n",
    "if schema_files:\n",
    "    # Try to initialize with the largest CSV file (likely the schema)\n",
    "    largest_csv = max(schema_files, key=lambda f: f.stat().st_size)\n",
    "    \n",
    "    try:\n",
    "        # Check if it has the right columns\n",
    "        df_sample = pd.read_csv(largest_csv, nrows=5)\n",
    "        required_cols = ['table_id', 'column', 'datatype']\n",
    "        \n",
    "        if all(col in df_sample.columns for col in required_cols):\n",
    "            print(f\"üîÑ Initializing schema manager with: {largest_csv.name}\")\n",
    "            schema_manager = SchemaManager(str(largest_csv), verbose=True)\n",
    "            print(f\"‚úÖ Schema manager loaded: {schema_manager.table_count} tables\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {largest_csv.name} doesn't have required schema columns\")\n",
    "            print(f\"   Found columns: {list(df_sample.columns)}\")\n",
    "            print(f\"   Required: {required_cols}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load schema manager: {e}\")\n",
    "\n",
    "if not schema_manager:\n",
    "    print(\"‚ö†Ô∏è No schema manager available - schema injection testing will be limited\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Embedding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_embeddings(queries: List[str], embeddings_model, vector_store=None):\n",
    "    \"\"\"\n",
    "    Analyze embeddings for a list of queries and show similarity patterns.\n",
    "    \"\"\"\n",
    "    if not embeddings_model:\n",
    "        print(\"‚ùå No embeddings model available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîÑ Generating embeddings for {len(queries)} queries...\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    query_embeddings = []\n",
    "    for i, query in enumerate(queries):\n",
    "        try:\n",
    "            embedding = embeddings_model.embed_query(query)\n",
    "            query_embeddings.append(embedding)\n",
    "            if i % 2 == 0:  # Progress indicator\n",
    "                print(f\"   {i+1}/{len(queries)} embeddings generated\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to embed query '{query[:50]}...': {e}\")\n",
    "            query_embeddings.append(None)\n",
    "    \n",
    "    # Filter out failed embeddings\n",
    "    valid_embeddings = [(q, e) for q, e in zip(queries, query_embeddings) if e is not None]\n",
    "    \n",
    "    if not valid_embeddings:\n",
    "        print(\"‚ùå No valid embeddings generated\")\n",
    "        return\n",
    "    \n",
    "    queries_valid = [q for q, e in valid_embeddings]\n",
    "    embeddings_array = np.array([e for q, e in valid_embeddings])\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(embeddings_array)} embeddings with {embeddings_array.shape[1]} dimensions\")\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    if SKLEARN_AVAILABLE:\n",
    "        similarity_matrix = cosine_similarity(embeddings_array)\n",
    "        \n",
    "        # Plot similarity heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(similarity_matrix, \n",
    "                   xticklabels=[q[:30] + '...' if len(q) > 30 else q for q in queries_valid],\n",
    "                   yticklabels=[q[:30] + '...' if len(q) > 30 else q for q in queries_valid],\n",
    "                   annot=True, fmt='.2f', cmap='coolwarm')\n",
    "        plt.title('Query Embedding Similarity Matrix')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # If vector store available, test retrieval\n",
    "    if vector_store:\n",
    "        print(\"\\nüîç Testing retrieval for each query:\")\n",
    "        for query in queries_valid[:3]:  # Test first 3 queries\n",
    "            print(f\"\\nüìù Query: {query[:60]}...\")\n",
    "            try:\n",
    "                docs = vector_store.similarity_search(query, k=3)\n",
    "                print(f\"   üìä Retrieved {len(docs)} documents\")\n",
    "                \n",
    "                for i, doc in enumerate(docs, 1):\n",
    "                    content_preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "                    source = doc.metadata.get('source', 'Unknown')\n",
    "                    print(f\"     {i}. {content_preview}... (Source: {source})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Retrieval failed: {e}\")\n",
    "    \n",
    "    return embeddings_array, queries_valid\n",
    "\n",
    "# Test with sample queries\n",
    "test_queries = [\n",
    "    \"How to join customer and order tables?\",\n",
    "    \"Show me LEFT JOIN examples with multiple tables\",\n",
    "    \"Customer analysis with aggregation functions\",\n",
    "    \"Inventory management queries\",\n",
    "    \"Calculate total sales by customer\",\n",
    "    \"Find top customers by order count\"\n",
    "]\n",
    "\n",
    "embedding_results = analyze_query_embeddings(test_queries, embeddings, vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval Pipeline Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_retrieval_pipeline(query: str, vector_store, k: int = 5, show_scores: bool = True):\n",
    "    \"\"\"\n",
    "    Detailed inspection of the retrieval pipeline for a single query.\n",
    "    \"\"\"\n",
    "    if not vector_store:\n",
    "        print(\"‚ùå No vector store available\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîç RETRIEVAL PIPELINE ANALYSIS\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Generate query embedding\n",
    "        print(\"\\n1Ô∏è‚É£ QUERY EMBEDDING GENERATION\")\n",
    "        start_time = time.time()\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        embed_time = time.time() - start_time\n",
    "        print(f\"   ‚úÖ Generated {len(query_embedding)}-dimensional embedding in {embed_time:.3f}s\")\n",
    "        print(f\"   üìä Embedding stats: min={min(query_embedding):.3f}, max={max(query_embedding):.3f}, mean={np.mean(query_embedding):.3f}\")\n",
    "        \n",
    "        # Step 2: Similarity search\n",
    "        print(\"\\n2Ô∏è‚É£ VECTOR SIMILARITY SEARCH\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if show_scores:\n",
    "            # Use similarity_search_with_score for detailed analysis\n",
    "            results_with_scores = vector_store.similarity_search_with_score(query, k=k)\n",
    "            search_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"   ‚úÖ Retrieved {len(results_with_scores)} documents in {search_time:.3f}s\")\n",
    "            \n",
    "            # Analyze results\n",
    "            docs = []\n",
    "            for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "                docs.append(doc)\n",
    "                print(f\"\\n   üìÑ Result {i} (Similarity Score: {score:.4f})\")\n",
    "                print(f\"      Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "                print(f\"      Content Preview: {doc.page_content[:150].replace(chr(10), ' ')}...\")\n",
    "                \n",
    "                # Show metadata\n",
    "                if doc.metadata:\n",
    "                    relevant_metadata = {k: v for k, v in doc.metadata.items() \n",
    "                                       if k in ['description', 'table', 'tables', 'joins'] and v}\n",
    "                    if relevant_metadata:\n",
    "                        print(f\"      Metadata: {relevant_metadata}\")\n",
    "        else:\n",
    "            docs = vector_store.similarity_search(query, k=k)\n",
    "            search_time = time.time() - start_time\n",
    "            print(f\"   ‚úÖ Retrieved {len(docs)} documents in {search_time:.3f}s\")\n",
    "        \n",
    "        # Step 3: Content analysis\n",
    "        print(\"\\n3Ô∏è‚É£ CONTENT ANALYSIS\")\n",
    "        total_content_length = sum(len(doc.page_content) for doc in docs)\n",
    "        print(f\"   üìä Total content length: {total_content_length:,} characters\")\n",
    "        \n",
    "        # Analyze query term overlap\n",
    "        query_terms = set(query.lower().split())\n",
    "        print(f\"   üîç Query terms: {query_terms}\")\n",
    "        \n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            content_terms = set(doc.page_content.lower().split())\n",
    "            overlap = query_terms.intersection(content_terms)\n",
    "            overlap_ratio = len(overlap) / len(query_terms) if query_terms else 0\n",
    "            print(f\"   üìÑ Doc {i}: {len(overlap)} matching terms ({overlap_ratio:.1%}) - {overlap}\")\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'query_embedding': query_embedding,\n",
    "            'retrieved_docs': docs,\n",
    "            'embed_time': embed_time,\n",
    "            'search_time': search_time,\n",
    "            'total_content_length': total_content_length\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Retrieval pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test the retrieval pipeline with different queries\n",
    "test_query = \"How to calculate inventory turnover using JOIN between inventory and sales tables?\"\n",
    "retrieval_results = inspect_retrieval_pipeline(test_query, vector_store, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Schema Injection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_schema_injection(query: str, retrieved_docs: List[Document], schema_manager=None):\n",
    "    \"\"\"\n",
    "    Analyze how schema injection works for a given query and retrieved documents.\n",
    "    \"\"\"\n",
    "    print(f\"üìã SCHEMA INJECTION ANALYSIS\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not schema_manager:\n",
    "        print(\"‚ö†Ô∏è No schema manager available - cannot test schema injection\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract tables from retrieved documents\n",
    "        print(\"\\n1Ô∏è‚É£ TABLE EXTRACTION FROM RETRIEVED DOCUMENTS\")\n",
    "        extracted_tables = schema_manager.extract_tables_from_documents(retrieved_docs)\n",
    "        print(f\"   üîç Extracted {len(extracted_tables)} table names: {extracted_tables}\")\n",
    "        \n",
    "        # Step 2: Analyze each document for table references\n",
    "        print(\"\\n2Ô∏è‚É£ DOCUMENT-LEVEL TABLE ANALYSIS\")\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            doc_tables = schema_manager.extract_tables_from_content(doc.page_content)\n",
    "            print(f\"   üìÑ Document {i}: {doc_tables}\")\n",
    "            print(f\"      Content sample: {doc.page_content[:100].replace(chr(10), ' ')}...\")\n",
    "            \n",
    "            # Check metadata for table information\n",
    "            if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                table_metadata = {k: v for k, v in doc.metadata.items() \n",
    "                                if k in ['table', 'tables', 'joins'] and v}\n",
    "                if table_metadata:\n",
    "                    print(f\"      Table metadata: {table_metadata}\")\n",
    "        \n",
    "        # Step 3: Schema lookup for extracted tables\n",
    "        print(\"\\n3Ô∏è‚É£ SCHEMA LOOKUP RESULTS\")\n",
    "        schema_found = []\n",
    "        schema_missing = []\n",
    "        \n",
    "        for table in extracted_tables:\n",
    "            schema = schema_manager.get_schema_for_table(table)\n",
    "            if schema:\n",
    "                schema_found.append((table, len(schema)))\n",
    "                print(f\"   ‚úÖ {table}: {len(schema)} columns found\")\n",
    "            else:\n",
    "                schema_missing.append(table)\n",
    "                print(f\"   ‚ùå {table}: No schema found\")\n",
    "        \n",
    "        # Step 4: Generate complete schema context\n",
    "        print(\"\\n4Ô∏è‚É£ GENERATED SCHEMA CONTEXT\")\n",
    "        if extracted_tables:\n",
    "            schema_context = schema_manager.get_relevant_schema(extracted_tables)\n",
    "            print(f\"   üìä Generated schema context ({len(schema_context)} characters):\")\n",
    "            print(\"   \" + \"-\" * 50)\n",
    "            print(schema_context[:500] + \"...\" if len(schema_context) > 500 else schema_context)\n",
    "            print(\"   \" + \"-\" * 50)\n",
    "        else:\n",
    "            schema_context = \"\"\n",
    "            print(\"   ‚ö†Ô∏è No schema context generated - no tables extracted\")\n",
    "        \n",
    "        # Step 5: Test with manual table names if extraction failed\n",
    "        if not extracted_tables:\n",
    "            print(\"\\n5Ô∏è‚É£ MANUAL TABLE EXTRACTION TEST\")\n",
    "            # Try common table names from the query\n",
    "            query_lower = query.lower()\n",
    "            common_tables = ['customers', 'orders', 'inventory', 'products', 'sales', 'users']\n",
    "            manual_tables = [t for t in common_tables if t in query_lower]\n",
    "            \n",
    "            if manual_tables:\n",
    "                print(f\"   üß™ Testing manual table names: {manual_tables}\")\n",
    "                manual_schema = schema_manager.get_relevant_schema(manual_tables)\n",
    "                if manual_schema:\n",
    "                    print(f\"   ‚úÖ Found schema for manual tables ({len(manual_schema)} chars)\")\n",
    "                    print(manual_schema[:200] + \"...\")\n",
    "                else:\n",
    "                    print(\"   ‚ùå No schema found for manual table names either\")\n",
    "        \n",
    "        return {\n",
    "            'extracted_tables': extracted_tables,\n",
    "            'schema_found': schema_found,\n",
    "            'schema_missing': schema_missing,\n",
    "            'schema_context': schema_context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Schema injection analysis failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test schema injection analysis\n",
    "if retrieval_results:\n",
    "    schema_analysis = analyze_schema_injection(\n",
    "        retrieval_results['query'], \n",
    "        retrieval_results['retrieved_docs'], \n",
    "        schema_manager\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LLM Context Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_llm_context(query: str, retrieved_docs: List[Document], schema_context: str = \"\"):\n",
    "    \"\"\"\n",
    "    Show exactly what context is being sent to the LLM.\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ LLM CONTEXT INSPECTOR\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Build document context\n",
    "    print(\"\\n1Ô∏è‚É£ DOCUMENT CONTEXT CONSTRUCTION\")\n",
    "    context_parts = []\n",
    "    total_doc_chars = 0\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        content = f\"Document {i}:\\n{doc.page_content}\"\n",
    "        \n",
    "        # Add metadata if available\n",
    "        metadata = doc.metadata\n",
    "        if metadata.get('description'):\n",
    "            content += f\"\\nDescription: {metadata['description']}\"\n",
    "        if metadata.get('table'):\n",
    "            content += f\"\\nTables: {metadata['table']}\"\n",
    "        if metadata.get('joins'):\n",
    "            content += f\"\\nJoins: {metadata['joins']}\"\n",
    "        \n",
    "        context_parts.append(content)\n",
    "        total_doc_chars += len(content)\n",
    "        print(f\"   üìÑ Document {i}: {len(content)} characters\")\n",
    "    \n",
    "    doc_context = \"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(context_parts)\n",
    "    print(f\"   üìä Total document context: {len(doc_context)} characters\")\n",
    "    \n",
    "    # Step 2: Add schema context\n",
    "    print(\"\\n2Ô∏è‚É£ SCHEMA CONTEXT ADDITION\")\n",
    "    if schema_context:\n",
    "        full_context = f\"{schema_context}\\n\\n{doc_context}\"\n",
    "        print(f\"   üìã Schema context: {len(schema_context)} characters\")\n",
    "        print(f\"   üìä Total context with schema: {len(full_context)} characters\")\n",
    "    else:\n",
    "        full_context = doc_context\n",
    "        print(f\"   ‚ö†Ô∏è No schema context available\")\n",
    "        print(f\"   üìä Total context: {len(full_context)} characters\")\n",
    "    \n",
    "    # Step 3: Build complete prompt\n",
    "    print(\"\\n3Ô∏è‚É£ COMPLETE PROMPT CONSTRUCTION\")\n",
    "    system_prompt = (\n",
    "        \"You are an expert SQL analyst helping answer questions about a retail analytics codebase. \"\n",
    "        \"Use ONLY the provided context to answer the user's question. If the answer is not contained \"\n",
    "        \"within the context, respond with 'I don't know based on the provided context.'\"\n",
    "    )\n",
    "    \n",
    "    complete_prompt = f\"{system_prompt}\\n\\nContext:\\n{full_context}\\n\\nUser question: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    print(f\"   üìù System prompt: {len(system_prompt)} characters\")\n",
    "    print(f\"   üéØ Complete prompt: {len(complete_prompt)} characters\")\n",
    "    \n",
    "    # Estimate token count (rough approximation)\n",
    "    estimated_tokens = len(complete_prompt.split()) * 1.3\n",
    "    print(f\"   üî¢ Estimated tokens: ~{int(estimated_tokens)}\")\n",
    "    \n",
    "    # Step 4: Show prompt sections\n",
    "    print(\"\\n4Ô∏è‚É£ PROMPT PREVIEW\")\n",
    "    print(\"   üìã System Prompt:\")\n",
    "    print(f\"      {system_prompt[:100]}...\")\n",
    "    \n",
    "    if schema_context:\n",
    "        print(\"   üìä Schema Context (first 200 chars):\")\n",
    "        print(f\"      {schema_context[:200].replace(chr(10), ' ')}...\")\n",
    "    \n",
    "    print(\"   üìÑ Document Context (first 200 chars):\")\n",
    "    print(f\"      {doc_context[:200].replace(chr(10), ' ')}...\")\n",
    "    \n",
    "    print(\"   ‚ùì User Query:\")\n",
    "    print(f\"      {query}\")\n",
    "    \n",
    "    return {\n",
    "        'system_prompt': system_prompt,\n",
    "        'document_context': doc_context,\n",
    "        'schema_context': schema_context,\n",
    "        'complete_prompt': complete_prompt,\n",
    "        'estimated_tokens': int(estimated_tokens),\n",
    "        'context_breakdown': {\n",
    "            'system_prompt_chars': len(system_prompt),\n",
    "            'document_context_chars': len(doc_context),\n",
    "            'schema_context_chars': len(schema_context),\n",
    "            'total_chars': len(complete_prompt)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test LLM context inspection\n",
    "if retrieval_results and schema_analysis:\n",
    "    llm_context_analysis = inspect_llm_context(\n",
    "        retrieval_results['query'],\n",
    "        retrieval_results['retrieved_docs'],\n",
    "        schema_analysis.get('schema_context', '')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. End-to-End Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_rag_pipeline(query: str, vector_store, schema_manager=None, k: int = 4):\n",
    "    \"\"\"\n",
    "    Run the complete RAG pipeline with full visibility.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ COMPLETE RAG PIPELINE TEST\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pipeline_results = {\n",
    "        'query': query,\n",
    "        'timestamps': {},\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Retrieve documents\n",
    "        print(\"\\nüìö STEP 1: DOCUMENT RETRIEVAL\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        retrieved_docs = vector_store.similarity_search(query, k=k)\n",
    "        pipeline_results['timestamps']['retrieval'] = time.time() - start_time\n",
    "        pipeline_results['retrieved_docs'] = retrieved_docs\n",
    "        \n",
    "        print(f\"   ‚úÖ Retrieved {len(retrieved_docs)} documents in {pipeline_results['timestamps']['retrieval']:.3f}s\")\n",
    "        \n",
    "        # Step 2: Extract tables and get schema\n",
    "        print(\"\\nüìã STEP 2: SCHEMA EXTRACTION\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        schema_context = \"\"\n",
    "        extracted_tables = []\n",
    "        \n",
    "        if schema_manager:\n",
    "            extracted_tables = schema_manager.extract_tables_from_documents(retrieved_docs)\n",
    "            schema_context = schema_manager.get_relevant_schema(extracted_tables)\n",
    "            \n",
    "            pipeline_results['timestamps']['schema'] = time.time() - start_time\n",
    "            pipeline_results['extracted_tables'] = extracted_tables\n",
    "            pipeline_results['schema_context'] = schema_context\n",
    "            \n",
    "            print(f\"   ‚úÖ Extracted {len(extracted_tables)} tables: {extracted_tables}\")\n",
    "            print(f\"   üìä Generated schema context: {len(schema_context)} characters\")\n",
    "            print(f\"   ‚è±Ô∏è Schema processing time: {pipeline_results['timestamps']['schema']:.3f}s\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No schema manager - skipping schema injection\")\n",
    "            pipeline_results['timestamps']['schema'] = 0\n",
    "        \n",
    "        # Step 3: Build context\n",
    "        print(\"\\nüîß STEP 3: CONTEXT CONSTRUCTION\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Build document context\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            content = f\"Document {i}:\\n{doc.page_content}\"\n",
    "            \n",
    "            # Add metadata\n",
    "            metadata = doc.metadata\n",
    "            if metadata.get('description'):\n",
    "                content += f\"\\nDescription: {metadata['description']}\"\n",
    "            if metadata.get('table'):\n",
    "                content += f\"\\nTables: {metadata['table']}\"\n",
    "            \n",
    "            context_parts.append(content)\n",
    "        \n",
    "        doc_context = \"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Combine with schema\n",
    "        if schema_context:\n",
    "            full_context = f\"{schema_context}\\n\\n{doc_context}\"\n",
    "        else:\n",
    "            full_context = doc_context\n",
    "        \n",
    "        pipeline_results['timestamps']['context'] = time.time() - start_time\n",
    "        pipeline_results['full_context'] = full_context\n",
    "        \n",
    "        print(f\"   üìä Context length: {len(full_context)} characters\")\n",
    "        print(f\"   ‚è±Ô∏è Context construction time: {pipeline_results['timestamps']['context']:.3f}s\")\n",
    "        \n",
    "        # Step 4: Generate answer with LLM\n",
    "        print(\"\\nü§ñ STEP 4: LLM GENERATION\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Try to generate answer using the LLM interaction module\n",
    "            answer, token_usage = generate_answer_from_context(\n",
    "                query=query,\n",
    "                context=full_context,\n",
    "                retries=2\n",
    "            )\n",
    "            \n",
    "            pipeline_results['timestamps']['llm'] = time.time() - start_time\n",
    "            pipeline_results['answer'] = answer\n",
    "            pipeline_results['token_usage'] = token_usage\n",
    "            \n",
    "            print(f\"   ‚úÖ Generated answer in {pipeline_results['timestamps']['llm']:.3f}s\")\n",
    "            print(f\"   üî¢ Token usage: {token_usage}\")\n",
    "            print(f\"   üìù Answer length: {len(answer)} characters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            pipeline_results['errors'].append(f\"LLM generation failed: {e}\")\n",
    "            print(f\"   ‚ùå LLM generation failed: {e}\")\n",
    "            pipeline_results['answer'] = None\n",
    "            pipeline_results['token_usage'] = {}\n",
    "        \n",
    "        # Step 5: Results summary\n",
    "        print(\"\\nüìä PIPELINE SUMMARY\")\n",
    "        total_time = sum(pipeline_results['timestamps'].values())\n",
    "        print(f\"   ‚è±Ô∏è Total pipeline time: {total_time:.3f}s\")\n",
    "        print(f\"      - Retrieval: {pipeline_results['timestamps']['retrieval']:.3f}s\")\n",
    "        print(f\"      - Schema: {pipeline_results['timestamps']['schema']:.3f}s\")\n",
    "        print(f\"      - Context: {pipeline_results['timestamps']['context']:.3f}s\")\n",
    "        print(f\"      - LLM: {pipeline_results['timestamps'].get('llm', 0):.3f}s\")\n",
    "        \n",
    "        if pipeline_results['answer']:\n",
    "            print(\"\\nüéØ GENERATED ANSWER:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(pipeline_results['answer'])\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        if pipeline_results['errors']:\n",
    "            print(\"\\n‚ö†Ô∏è ERRORS ENCOUNTERED:\")\n",
    "            for error in pipeline_results['errors']:\n",
    "                print(f\"   - {error}\")\n",
    "        \n",
    "        return pipeline_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_results['errors'].append(f\"Pipeline error: {e}\")\n",
    "        print(f\"‚ùå Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pipeline_results\n",
    "\n",
    "# Test the complete pipeline\n",
    "test_pipeline_query = \"Show me how to calculate customer lifetime value using SQL joins\"\n",
    "complete_pipeline_results = run_complete_rag_pipeline(\n",
    "    test_pipeline_query, \n",
    "    vector_store, \n",
    "    schema_manager, \n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Query Testing Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ipywidgets if not available\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ipywidgets not available - install with: pip install ipywidgets\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "if WIDGETS_AVAILABLE:\n",
    "    def create_interactive_tester():\n",
    "        \"\"\"\n",
    "        Create an interactive widget for testing queries.\n",
    "        \"\"\"\n",
    "        # Create widgets\n",
    "        query_input = widgets.Textarea(\n",
    "            value=\"How to join customer and order tables?\",\n",
    "            placeholder=\"Enter your test query here...\",\n",
    "            description=\"Query:\",\n",
    "            layout=widgets.Layout(width='80%', height='80px')\n",
    "        )\n",
    "        \n",
    "        k_slider = widgets.IntSlider(\n",
    "            value=3,\n",
    "            min=1,\n",
    "            max=10,\n",
    "            description=\"Retrieve (k):\",\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        test_button = widgets.Button(\n",
    "            description=\"üß™ Test Query\",\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        def test_query_interactive(b):\n",
    "            with output_area:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                query = query_input.value.strip()\n",
    "                k = k_slider.value\n",
    "                \n",
    "                if not query:\n",
    "                    print(\"‚ùå Please enter a query\")\n",
    "                    return\n",
    "                \n",
    "                if not vector_store:\n",
    "                    print(\"‚ùå No vector store available\")\n",
    "                    return\n",
    "                \n",
    "                print(f\"üß™ Testing query: '{query}' (k={k})\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                try:\n",
    "                    # Quick retrieval test\n",
    "                    start = time.time()\n",
    "                    docs = vector_store.similarity_search(query, k=k)\n",
    "                    retrieval_time = time.time() - start\n",
    "                    \n",
    "                    print(f\"üìä Retrieved {len(docs)} documents in {retrieval_time:.3f}s\\n\")\n",
    "                    \n",
    "                    # Show results\n",
    "                    for i, doc in enumerate(docs, 1):\n",
    "                        print(f\"üìÑ Result {i}:\")\n",
    "                        print(f\"   Content: {doc.page_content[:120]}...\")\n",
    "                        print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "                        print()\n",
    "                    \n",
    "                    # Schema extraction if available\n",
    "                    if schema_manager:\n",
    "                        tables = schema_manager.extract_tables_from_documents(docs)\n",
    "                        if tables:\n",
    "                            print(f\"üè∑Ô∏è Extracted tables: {tables}\")\n",
    "                            schema = schema_manager.get_relevant_schema(tables[:3])  # Limit for display\n",
    "                            if schema:\n",
    "                                print(f\"üìã Schema context: {len(schema)} characters\")\n",
    "                        else:\n",
    "                            print(\"‚ö†Ô∏è No tables extracted from retrieved documents\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Test failed: {e}\")\n",
    "        \n",
    "        test_button.on_click(test_query_interactive)\n",
    "        \n",
    "        # Layout\n",
    "        controls = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üîç Interactive Query Tester</h3>\"),\n",
    "            query_input,\n",
    "            widgets.HBox([k_slider, test_button]),\n",
    "            output_area\n",
    "        ])\n",
    "        \n",
    "        return controls\n",
    "    \n",
    "    # Create and display the interactive tester\n",
    "    if vector_store:\n",
    "        interactive_tester = create_interactive_tester()\n",
    "        display(interactive_tester)\n",
    "    else:\n",
    "        print(\"‚ùå Cannot create interactive tester - no vector store available\")\n",
    "        \n",
    "else:\n",
    "    print(\"üìù Manual testing section - modify the cell below to test different queries:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Analysis & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_system_performance():\n",
    "    \"\"\"\n",
    "    Analyze the performance characteristics of your RAG system.\n",
    "    \"\"\"\n",
    "    print(\"üìà RAG SYSTEM PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    analysis = {\n",
    "        'vector_store_info': {},\n",
    "        'embedding_performance': {},\n",
    "        'retrieval_performance': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Vector store analysis\n",
    "    if vector_store:\n",
    "        print(\"\\nüóÑÔ∏è VECTOR STORE ANALYSIS\")\n",
    "        num_docs = len(vector_store.docstore._dict)\n",
    "        analysis['vector_store_info'] = {\n",
    "            'total_documents': num_docs,\n",
    "            'embedding_dimension': len(embeddings.embed_query(\"test\")) if embeddings else \"Unknown\"\n",
    "        }\n",
    "        \n",
    "        print(f\"   üìä Total documents: {num_docs:,}\")\n",
    "        if embeddings:\n",
    "            print(f\"   üìè Embedding dimension: {analysis['vector_store_info']['embedding_dimension']}\")\n",
    "        \n",
    "        # Sample document analysis\n",
    "        sample_docs = list(vector_store.docstore._dict.values())[:5]\n",
    "        if sample_docs:\n",
    "            doc_lengths = [len(doc.page_content) for doc in sample_docs]\n",
    "            avg_length = sum(doc_lengths) / len(doc_lengths)\n",
    "            print(f\"   üìÑ Average document length: {avg_length:.0f} characters\")\n",
    "            analysis['vector_store_info']['avg_doc_length'] = avg_length\n",
    "    \n",
    "    # Embedding performance test\n",
    "    if embeddings:\n",
    "        print(\"\\n‚ö° EMBEDDING PERFORMANCE TEST\")\n",
    "        test_queries = [\n",
    "            \"short query\",\n",
    "            \"This is a medium length query with several words to test embedding performance\",\n",
    "            \"This is a much longer query that contains multiple sentences and various SQL-related terms like SELECT, JOIN, WHERE, GROUP BY, and ORDER BY to simulate real user queries that might be submitted to the RAG system for processing and analysis.\"\n",
    "        ]\n",
    "        \n",
    "        embedding_times = []\n",
    "        for i, query in enumerate(test_queries):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                embeddings.embed_query(query)\n",
    "                embed_time = time.time() - start\n",
    "                embedding_times.append(embed_time)\n",
    "                print(f\"   Query {i+1} ({len(query)} chars): {embed_time:.3f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Query {i+1}: Failed - {e}\")\n",
    "        \n",
    "        if embedding_times:\n",
    "            avg_embedding_time = sum(embedding_times) / len(embedding_times)\n",
    "            analysis['embedding_performance'] = {\n",
    "                'avg_time': avg_embedding_time,\n",
    "                'min_time': min(embedding_times),\n",
    "                'max_time': max(embedding_times)\n",
    "            }\n",
    "            print(f\"   üìä Average embedding time: {avg_embedding_time:.3f}s\")\n",
    "    \n",
    "    # Retrieval performance test\n",
    "    if vector_store:\n",
    "        print(\"\\nüîç RETRIEVAL PERFORMANCE TEST\")\n",
    "        test_k_values = [1, 3, 5, 10]\n",
    "        test_query = \"customer order analysis with joins\"\n",
    "        \n",
    "        retrieval_times = {}\n",
    "        for k in test_k_values:\n",
    "            try:\n",
    "                start = time.time()\n",
    "                docs = vector_store.similarity_search(test_query, k=k)\n",
    "                retrieval_time = time.time() - start\n",
    "                retrieval_times[k] = retrieval_time\n",
    "                print(f\"   k={k}: {retrieval_time:.3f}s ({len(docs)} docs)\")\n",
    "            except Exception as e:\n",
    "                print(f\"   k={k}: Failed - {e}\")\n",
    "        \n",
    "        analysis['retrieval_performance'] = retrieval_times\n",
    "    \n",
    "    # Schema manager performance\n",
    "    if schema_manager:\n",
    "        print(\"\\nüìã SCHEMA MANAGER ANALYSIS\")\n",
    "        stats = schema_manager.get_schema_stats()\n",
    "        print(f\"   üìä Total tables: {stats['total_tables']:,}\")\n",
    "        print(f\"   üìä Total columns: {stats['total_columns']:,}\")\n",
    "        print(f\"   üìä Average columns per table: {stats['avg_columns_per_table']:.1f}\")\n",
    "        \n",
    "        # Test table extraction performance\n",
    "        test_content = \"SELECT * FROM customers c JOIN orders o ON c.customer_id = o.customer_id JOIN products p ON o.product_id = p.product_id\"\n",
    "        start = time.time()\n",
    "        tables = schema_manager.extract_tables_from_content(test_content)\n",
    "        extraction_time = time.time() - start\n",
    "        print(f\"   üîç Table extraction time: {extraction_time:.4f}s ({len(tables)} tables found)\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    print(\"\\nüí° PERFORMANCE RECOMMENDATIONS\")\n",
    "    recommendations = []\n",
    "    \n",
    "    if vector_store and analysis['vector_store_info'].get('total_documents', 0) > 10000:\n",
    "        recommendations.append(\"Consider implementing hierarchical indexing for large document collections\")\n",
    "    \n",
    "    if analysis.get('embedding_performance', {}).get('avg_time', 0) > 1.0:\n",
    "        recommendations.append(\"Embedding generation is slow - consider using a faster embedding model or caching\")\n",
    "    \n",
    "    if analysis.get('retrieval_performance'):\n",
    "        retrieval_times = analysis['retrieval_performance']\n",
    "        if retrieval_times.get(5, 0) > 0.5:\n",
    "            recommendations.append(\"Retrieval is slow - consider optimizing FAISS index or reducing k values\")\n",
    "    \n",
    "    if not schema_manager:\n",
    "        recommendations.append(\"No schema manager detected - consider implementing smart schema injection\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        recommendations.append(\"System performance looks good! Consider monitoring in production.\")\n",
    "    \n",
    "    analysis['recommendations'] = recommendations\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Run performance analysis\n",
    "performance_analysis = analyze_system_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Debug Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_debug_results():\n",
    "    \"\"\"\n",
    "    Export all debug results to JSON for further analysis.\n",
    "    \"\"\"\n",
    "    print(\"üíæ EXPORTING DEBUG RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Collect all results\n",
    "    debug_export = {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'system_info': {\n",
    "            'vector_store_available': vector_store is not None,\n",
    "            'schema_manager_available': schema_manager is not None,\n",
    "            'embeddings_available': embeddings is not None,\n",
    "            'sklearn_available': SKLEARN_AVAILABLE,\n",
    "            'umap_available': UMAP_AVAILABLE,\n",
    "            'widgets_available': WIDGETS_AVAILABLE\n",
    "        },\n",
    "        'retrieval_results': retrieval_results if 'retrieval_results' in locals() else None,\n",
    "        'schema_analysis': schema_analysis if 'schema_analysis' in locals() else None,\n",
    "        'llm_context_analysis': llm_context_analysis if 'llm_context_analysis' in locals() else None,\n",
    "        'complete_pipeline_results': complete_pipeline_results if 'complete_pipeline_results' in locals() else None,\n",
    "        'performance_analysis': performance_analysis if 'performance_analysis' in locals() else None\n",
    "    }\n",
    "    \n",
    "    # Clean up non-serializable objects\n",
    "    def clean_for_json(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: clean_for_json(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [clean_for_json(item) for item in obj]\n",
    "        elif hasattr(obj, 'page_content'):  # Document object\n",
    "            return {\n",
    "                'page_content': str(obj.page_content)[:500] + '...' if len(str(obj.page_content)) > 500 else str(obj.page_content),\n",
    "                'metadata': dict(obj.metadata) if hasattr(obj, 'metadata') else {}\n",
    "            }\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return f\"numpy_array_shape_{obj.shape}\"\n",
    "        elif callable(obj):\n",
    "            return f\"callable_{obj.__class__.__name__}\"\n",
    "        else:\n",
    "            try:\n",
    "                json.dumps(obj)  # Test if serializable\n",
    "                return obj\n",
    "            except (TypeError, ValueError):\n",
    "                return str(obj)\n",
    "    \n",
    "    cleaned_export = clean_for_json(debug_export)\n",
    "    \n",
    "    # Save to file\n",
    "    export_filename = f\"rag_debug_results_{int(time.time())}.json\"\n",
    "    export_path = current_dir / export_filename\n",
    "    \n",
    "    try:\n",
    "        with open(export_path, 'w') as f:\n",
    "            json.dump(cleaned_export, f, indent=2)\n",
    "        \n",
    "        file_size = export_path.stat().st_size / 1024  # KB\n",
    "        print(f\"‚úÖ Debug results exported to: {export_filename}\")\n",
    "        print(f\"üìä File size: {file_size:.1f} KB\")\n",
    "        \n",
    "        # Show summary\n",
    "        print(\"\\nüìã Export Summary:\")\n",
    "        for key, value in debug_export.items():\n",
    "            if key == 'system_info':\n",
    "                available_components = sum(1 for v in value.values() if v)\n",
    "                print(f\"   {key}: {available_components}/{len(value)} components available\")\n",
    "            elif value is not None:\n",
    "                print(f\"   {key}: ‚úÖ Included\")\n",
    "            else:\n",
    "                print(f\"   {key}: ‚ùå Not available\")\n",
    "        \n",
    "        return export_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to export results: {e}\")\n",
    "        return None\n",
    "\n",
    "# Export results\n",
    "export_path = export_debug_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "This notebook provides comprehensive debugging capabilities for your RAG system. Here's what you can use it for:\n",
    "\n",
    "### üîç **Debugging Capabilities**\n",
    "1. **Vector Embedding Analysis** - See what embeddings are generated and their similarity patterns\n",
    "2. **Retrieval Pipeline Inspection** - Understand exactly which documents are retrieved and why\n",
    "3. **Schema Injection Analysis** - Debug why certain tables aren't being found or extracted\n",
    "4. **LLM Context Inspector** - See the exact prompt being sent to your language model\n",
    "5. **End-to-End Testing** - Run complete pipeline with full visibility\n",
    "6. **Performance Analysis** - Identify bottlenecks and optimization opportunities\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "1. **Run different test queries** to understand retrieval patterns\n",
    "2. **Experiment with k values** to optimize retrieval count\n",
    "3. **Test schema injection** with queries containing table names\n",
    "4. **Analyze failed queries** to understand system limitations\n",
    "5. **Use exported results** for deeper analysis and system tuning\n",
    "\n",
    "### üí° **Common Issues & Solutions**\n",
    "- **No tables extracted**: Check if your documents contain proper SQL syntax or table metadata\n",
    "- **Poor retrieval**: Consider adjusting embedding model or similarity thresholds\n",
    "- **Slow performance**: Use the performance analysis to identify bottlenecks\n",
    "- **LLM failures**: Check context length and ensure proper prompt formatting\n",
    "\n",
    "**This notebook is your debugging companion - modify and extend it as needed for your specific use cases!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
